{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4b5e4f-d979-4ff9-bc74-9262d8bae425",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c25d7-89e5-4ae4-858c-9b13afed4f2d",
   "metadata": {},
   "source": [
    "## Conceptual Questions\n",
    "\n",
    "### Markov Assumption\n",
    "It is based on the idea that the future state of a system depends only on its present state. In easier words, what happens next only depends on what situation you are right now.\n",
    "\n",
    "An example could be robot docking to the charger. The robot cares only on its current position with respect to the charger, instead of taking into account all the previous positions.\n",
    "\n",
    "### Markov Decision Process\n",
    "In an MDP, we have a decision maker, called an agent, that interacts with the environment. The interactions occur sequentially over time. \n",
    "\n",
    "#### Components of RL used in MDP:\n",
    "- Agents (A) are the central concept of the field of RL. \n",
    "- Environment (E) is a well-defined and structured system that agents interact with over time.\n",
    "- States (S) provide information about current context of the environment. States represent different situations, configurations, or conditions agent can be in.\n",
    "- Actions (A) are decisions made by the agent to influance the stae of the environment. There are a set of permissible actions that the RL agent can takel\n",
    "- Rewards (R) is a numerical signal the agent receieves after each action. The goal of the model is to maximize the overall reward.\n",
    "- Termination Condition: end of episode based on reaching a particular state, a certain number of time steps, or some other scriteria.\n",
    "- Observations are feedback from the environment to the agent. \n",
    "\n",
    "\n",
    "### Markov Reward Process\n",
    "A simplified version of MDP where there are no actions to choose from. \n",
    "It is defin4ed by the following components:\n",
    "    1. States (s)\n",
    "    2. Transition Probabilities (P)\n",
    "    3. Reward Function (R)\n",
    "    4. Discount Factor ($\\gamma$)\n",
    "\n",
    "In order to reduce MDP to MRP, we fix the policy $\\pi$ and replace the action-based transition with a state-only transition.\n",
    "\n",
    "### Policy Definition\n",
    "A policy is a set of instructions that tells an agent what action to take in each state. \n",
    "- Deterministic Policy is a type of policy that always chooses the same action for a given state.\n",
    "- Stochastic Policy is a type of policy that chooses randomly according to the probability distribution over actions. \n",
    "\n",
    "\n",
    "### Infinite Horizon Value Function\n",
    "Its goal is to computye the total expected reward the agent receives over an infinte number of steps into the futuer. \n",
    "The discount factor $\\gamma \\in [0, 1)$ controls how much future rewards are worth compared to immediate rewards. \n",
    "- if $\\gamma$ is close to 0, the agent focuses on immediate rewards\n",
    "- if $\\gamma$ is close to 1, the agent values future rewards more\n",
    "\n",
    "If $\\gamma \\ge 1$, the function most probably will diverge resulting the grow of uncontrollability. \n",
    "\n",
    "\n",
    "### Policy Iteration\n",
    "It is an algorithm that is used to find optimal policy in MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41be69-1d9f-4cfd-a8cf-7e399af91e4d",
   "metadata": {},
   "source": [
    "## Analytical Problems\n",
    "\n",
    "1. Value Function Computation:\n",
    "\n",
    "$$S = \\{A, B\\}$$\n",
    "\n",
    "$$\\gamma = 0.9$$\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "0.5 & 0.5 \\\\\n",
    "0.4 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix}\n",
    "5 \\\\\n",
    "2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Solution:\n",
    "\n",
    "$$ V = R + \\gamma PV \n",
    "$$\n",
    "\n",
    "$$ (I - \\gamma P) V = R $$\n",
    "\n",
    "\n",
    "$$ V_A = 5 + 0.9(0.5V_A + 0.5V_B) = 5 + 0.45V_A + 0.45V_B $$\n",
    "$$ V_B = 2 + 0.9(0.4V_A + 0.6V_B) = 2 + 0.36V_A + 0.54V_B $$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-0.55 & -0.45 \\\\\n",
    "-0.36 & -0.46\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "V_A\\\\\n",
    "V_B\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "5\\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ V_A = 35.16 \\quad V_B = 31.86$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0039d6d5-2411-4db8-b83b-e0a0b0517948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: [5. 2.]\n",
      "Iteration 2: [8.15 4.88]\n",
      "Iteration 3: [10.8635  7.5692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "gamma = 0.9\n",
    "\n",
    "R = np.array([5, 2])\n",
    "\n",
    "P = np.array([\n",
    "    [0.5, 0.5],\n",
    "    [0.4, 0.6]\n",
    "])\n",
    "\n",
    "V = np.array([0.0, 0.0])\n",
    "\n",
    "for k in range(3):\n",
    "    V = R + gamma * P.dot(V)\n",
    "    print(f\"Iteration {k + 1}: {V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9631187f-0e0a-4bbf-80c4-aa295cfea901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_mdp_data(f_dir):\n",
    "    with open(f_dir, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def init_rand_policy(states, actions):\n",
    "    policy = {s: np.random.choice(actions) for s in states}\n",
    "    return policy\n",
    "\n",
    "def policy_evaluation(policy, transitions, states, gamma, theta=1e-6):\n",
    "    V = {s: 0 for s in states}\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            a = policy[s]\n",
    "            v_new = 0\n",
    "            for trans in transitions[str(s)][a]:\n",
    "                ns = trans[\"next_state\"]\n",
    "                prob = trans[\"prob\"]\n",
    "                reward = trans[\"reward\"]\n",
    "                v_new += prob * (reward + gamma * V[ns])\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdeba998-14a3-4c6b-ae15-1208a27cb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, states, actions, transitions, gamma, policy):\n",
    "    policy_stable = True\n",
    "    for s in states:\n",
    "        old_action = policy[s]\n",
    "        action_values = {}\n",
    "        for a in actions:\n",
    "            v = 0\n",
    "            for trans in transitions[str(s)][a]:\n",
    "                ns = trans[\"next_state\"]\n",
    "                prob = trans[\"prob\"]\n",
    "                reward = trans[\"reward\"]\n",
    "                v += prob * (reward + gamma * V[ns])\n",
    "            action_values[a] = v\n",
    "        best_action = max(action_values, key=action_values.get)\n",
    "        policy[s] = best_action\n",
    "        if best_action != old_action:\n",
    "            policy_stable = False\n",
    "    return policy_stable, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb79ae26-7e22-41c8-9734-175ed1667269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp_file):\n",
    "    mdp_data = load_mdp_data(mdp_file)\n",
    "    states = mdp_data[\"states\"]\n",
    "    actions = mdp_data[\"actions\"]\n",
    "    gamma = mdp_data[\"discount_factor\"]\n",
    "    transitions = mdp_data[\"transitions\"]\n",
    "\n",
    "    policy = init_rand_policy(states, actions)\n",
    "    print(\"rand policy:\", policy)\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        print(f\"\\nPolicy Iteration: {iteration}\")\n",
    "        V = policy_evaluation(policy, transitions, states, gamma)\n",
    "        stable, policy = policy_improvement(V, states, actions, transitions, gamma, policy)\n",
    "        print(f\"Policy after iteration {iteration}: {policy}\")\n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "    print(\"\\nOptimal policy:\", policy)\n",
    "    print(\"Optimal value function:\")\n",
    "    for s in states:\n",
    "        print(f\"V({s}) = {V[s]:.4f}\")\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42afbc04-c462-4cea-b3d1-17d982680c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand policy: {0: np.str_('right'), 1: np.str_('left'), 2: np.str_('left')}\n",
      "\n",
      "Policy Iteration: 1\n",
      "Policy after iteration 1: {0: 'left', 1: 'right', 2: 'left'}\n",
      "\n",
      "Policy Iteration: 2\n",
      "Policy after iteration 2: {0: 'left', 1: 'right', 2: 'left'}\n",
      "\n",
      "Optimal policy: {0: 'left', 1: 'right', 2: 'left'}\n",
      "Optimal value function:\n",
      "V(0) = 14.3523\n",
      "V(1) = 14.4828\n",
      "V(2) = 12.7586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: 'left', 1: 'right', 2: 'left'},\n",
       " {0: 14.352277771003633, 1: 14.482753998358886, 2: 12.758616298440941})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_file = \"assignments/mdp_data.json\"\n",
    "policy_iteration(mdp_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
