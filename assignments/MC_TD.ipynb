{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b09582ac-09ed-43c0-a163-6083d9f28791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c210e43-4f72-406a-8897-0f970bcbf55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld = np.array([\n",
    "    ['S', 'F', 'F', 'F'],\n",
    "    ['F', 'H', 'F', 'H'],\n",
    "    ['F', 'F', 'F', 'H'],\n",
    "    ['H', 'F', 'F', 'G']\n",
    "])\n",
    "\n",
    "rewards = {\n",
    "    'S': 0,\n",
    "    'G': 1,\n",
    "    'H': -1,\n",
    "    'F': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ddfd67fe-2684-4b9a-9de8-88154c349853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(grid, rewards, num_episodes, gamma):\n",
    "    state_values = np.zeros(grid.shape)\n",
    "    state_counts = np.zeros(grid.shape)\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode = []\n",
    "        state = (0, 0)  # Starting state\n",
    "\n",
    "        # Generate an episode by following a random policy\n",
    "        while grid[state] != 'G':\n",
    "            action = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "            next_state = get_next_state(state, action)\n",
    "\n",
    "            episode.append((state, action, rewards[grid[next_state]]))\n",
    "            state = next_state\n",
    "\n",
    "        # Update state values using Monte Carlo returns\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            state_counts[state] += 1\n",
    "            state_values[state] += (G - state_values[state]) / state_counts[state]\n",
    "\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea7eba56-b3d6-4712-9c32-7db9a6f5288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_learning(grid, rewards, num_episodes, alpha=0.1, gamma=0.9):\n",
    "    state_values = np.zeros(grid.shape)\n",
    "    returns_per_episode = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = (0, 0)  \n",
    "        total_return = 0\n",
    "\n",
    "        while grid[state] != 'G':  \n",
    "            action = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "            next_state = get_next_state(state, action)\n",
    "            reward = rewards[grid[next_state]]\n",
    "\n",
    "            # TD update\n",
    "            state_values[state] += alpha * (reward + gamma * state_values[next_state] - state_values[state])\n",
    "\n",
    "            \n",
    "            state = next_state\n",
    "            total_return += reward\n",
    "\n",
    "        returns_per_episode.append(total_return)\n",
    "\n",
    "    return state_values, returns_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09f5301c-3051-4cc2-a8e8-625940e5efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    if action == 'up':\n",
    "        return (max(state[0] - 1, 0), state[1])\n",
    "    elif action == 'down':\n",
    "        return (min(state[0] + 1, gridworld.shape[0] - 1), state[1])\n",
    "    elif action == 'left':\n",
    "        return (state[0], max(state[1] - 1, 0))\n",
    "    elif action == 'right':\n",
    "        return (state[0], min(state[1] + 1, gridworld.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aba2b1a9-4ec3-4272-88a3-74c79bc51919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo State Values:\n",
      "[[-11.98076781 -11.88467282 -11.29607526 -11.16988706]\n",
      " [-12.06034542 -11.37502866 -10.96089234 -10.18310925]\n",
      " [-11.82778031 -10.68142204  -8.75809417  -6.5274746 ]\n",
      " [-12.02321973  -9.99358508  -5.78725658   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Test the algorithms\n",
    "num_episodes = 10000\n",
    "gamma = 0.999\n",
    "alpha = 0.001\n",
    "\n",
    "print(\"Monte Carlo State Values:\")\n",
    "mc_state_values = monte_carlo(gridworld, rewards, num_episodes, gamma)\n",
    "print(mc_state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "152cff37-dc2e-40c3-879f-b772fcbd78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(state, state_values, grid):\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    best_action = None\n",
    "    best_value = -np.inf\n",
    "\n",
    "    for action in actions:\n",
    "        next_state = get_next_state(state, action)\n",
    "        if next_state == state:\n",
    "            continue\n",
    "        value = state_values[next_state]\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_action = action\n",
    "\n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3d6c95f-2f2e-4ad3-a294-1283a30c0346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached!\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "cell_size = 100\n",
    "rows, cols = gridworld.shape\n",
    "screen = pygame.display.set_mode((cols * cell_size, rows * cell_size))\n",
    "pygame.display.set_caption(\"Gridworld Simulation\")\n",
    "\n",
    "# Colors\n",
    "colors = {\n",
    "    'S': (0, 255, 0),   \n",
    "    'G': (255, 215, 0), \n",
    "    'H': (255, 0, 0),   \n",
    "    'F': (200, 200, 200), \n",
    "}\n",
    "agent_color = (0, 0, 255) \n",
    "\n",
    "# Simulation\n",
    "state = (0, 0)\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    #grid\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            rect = pygame.Rect(j * cell_size, i * cell_size, cell_size, cell_size)\n",
    "            pygame.draw.rect(screen, colors[gridworld[i, j]], rect)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect, 2)\n",
    "\n",
    "    #agent\n",
    "    x, y = state[1] * cell_size + 10, state[0] * cell_size + 10\n",
    "    pygame.draw.circle(screen, agent_color, (x + 40, y + 40), 20)\n",
    "\n",
    "    pygame.display.flip()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if gridworld[state] == 'H':\n",
    "        print(\"Robot fell into hole!\")\n",
    "        time.sleep(1)\n",
    "        break\n",
    "\n",
    "    if gridworld[state] == 'G':\n",
    "        print(\"Goal reached!\")\n",
    "        time.sleep(1)\n",
    "        break\n",
    "\n",
    "    # Move according to greedy policy\n",
    "    action = greedy_action(state, mc_state_values, gridworld)\n",
    "    next_state = get_next_state(state, action)\n",
    "    state = next_state\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcae0ec-f499-4097-aa8c-2a3f1aaa38c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
