{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa42df0f-22e2-45a5-8ed3-78bd7c004a51",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "### Offline RL motivation\n",
    "Offline RL is important, because it is hard or expenisve to simulate some process. That is why we use recorded and logged real data from user's experience or experts moves etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99445cc-2be9-4433-a1cc-59b0be5bbf0a",
   "metadata": {},
   "source": [
    "### Policy Evaluation in Offline RL\n",
    "In online policy evaluation we can evalaute it imidieatly by reward cause we simulate our policy in environment.\n",
    "In offline policy evaluation we can not directly and fully evaluate policy based on our fixed dataset, because it might go the other way or learn rare things that we can't adequeatly evalurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f86e2-a6f3-4c8c-b354-33c6c3ea631a",
   "metadata": {},
   "source": [
    "### Distribution Shift\n",
    "The distribution of states and actions that Ï€ would visit is DIFFERENT from the distribution of states and actions in your dataset D "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e25b68-c176-4c6b-b91e-3e18161a3bfe",
   "metadata": {},
   "source": [
    "### Extrapolation Error\n",
    "It is the error during distribution shift is when we try to estimate unseen data that goes beyond our dataset range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a1a4e5-8e2d-4a11-bfcc-a1d8221c7a53",
   "metadata": {},
   "source": [
    "### Fitted Q-Evaluation\n",
    "FQE is a way to estimate Q-function for policy using only your offline dataset. We turn the Q-function estimation problem into a supervised learning regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729285f-3125-4e12-95c3-df1e6a4e94b0",
   "metadata": {},
   "source": [
    "### Offline policy optimization\n",
    "To optimize policy to find the best policy using only fixed dataset without any interaction with the environment.\n",
    "Behavior Cloning just to clone existing policy/expert\n",
    "Batch-Constrained Q-learning only consider actions that are similar to what's in the dataset, but optimize among those ac# Part B - Analytical Problems Solutionstions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534d4db-9e56-4a04-8571-4852d712ddda",
   "metadata": {},
   "source": [
    "# Part B - Analytical Problems Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d378989-f7e5-4784-80bb-aa437b1c1ac6",
   "metadata": {},
   "source": [
    "## Problem 7: Importance Sampling for Offline Evaluation (10 pts)\n",
    "\n",
    "**Given:**\n",
    "- $s_0, a_0, r_1 = 1, \\pi(a_0|s_0) = 0.6, \\mu(a_0|s_0) = 0.3$\n",
    "- $s_1, a_1, r_2 = 2, \\pi(a_1|s_1) = 0.4, \\mu(a_1|s_1) = 0.2$\n",
    "- $\\gamma = 0.9$\n",
    "\n",
    "### Solution:\n",
    "\n",
    "**Step 1: Calculate the return $G_0$**\n",
    "\n",
    "$$G_0 = r_1 + \\gamma r_2$$\n",
    "\n",
    "$$G_0 = 1 + 0.9 \\times 2$$\n",
    "\n",
    "$$G_0 = 1 + 1.8 = 2.8$$\n",
    "\n",
    "**Step 2: Calculate the importance sampling ratio**\n",
    "\n",
    "$$\\rho = \\frac{\\pi(a_0|s_0)}{\\mu(a_0|s_0)} \\times \\frac{\\pi(a_1|s_1)}{\\mu(a_1|s_1)}$$\n",
    "\n",
    "$$\\rho = \\frac{0.6}{0.3} \\times \\frac{0.4}{0.2}$$\n",
    "\n",
    "$$\\rho = 2 \\times 2 = 4$$\n",
    "\n",
    "**Step 3: Calculate the ordinary importance sampling estimate**\n",
    "\n",
    "$$\\hat{V}(s_0) = \\rho \\times G_0$$\n",
    "\n",
    "$$\\hat{V}(s_0) = 4 \\times 2.8 = 11.2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fc432-6b2f-449f-86f8-7965c329f12c",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "**Given:**\n",
    "- Dataset: $(s = 1, a = A, r = 2, s' = 2)$, $(s = 2, a = B, r = 0, s' = 1)$\n",
    "- Current Q-values: $Q(1, A) = 5$, $Q(2, B) = 4$\n",
    "- Discount $\\gamma = 0.95$, learning rate $\\alpha = 0.1$\n",
    "- $\\max_{a'} Q(s', a') = 6$ for $s' = 2$ and $5$ for $s' = 1$\n",
    "\n",
    "**Task:** Perform one batch Q-learning update for both tuples\n",
    "\n",
    "### Solution:\n",
    "\n",
    "**Q-learning update formula:**\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "### Tuple 1: $(s = 1, a = A, r = 2, s' = 2)$\n",
    "\n",
    "**Calculate target:**\n",
    "\n",
    "$$\\text{Target} = r + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "$$\\text{Target} = 2 + 0.95 \\times 6 = 2 + 5.7 = 7.7$$\n",
    "\n",
    "**Update Q-value:**\n",
    "\n",
    "$$Q(1, A) \\leftarrow 5 + 0.1 \\times (7.7 - 5)$$\n",
    "\n",
    "$$Q(1, A) \\leftarrow 5 + 0.1 \\times 2.7$$\n",
    "\n",
    "$$Q(1, A) \\leftarrow 5 + 0.27 = 5.27$$\n",
    "\n",
    "### Tuple 2: $(s = 2, a = B, r = 0, s' = 1)$\n",
    "\n",
    "**Calculate target:**\n",
    "\n",
    "$$\\text{Target} = r + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "$$\\text{Target} = 0 + 0.95 \\times 5 = 4.75$$\n",
    "\n",
    "**Update Q-value:**\n",
    "\n",
    "$$Q(2, B) \\leftarrow 4 + 0.1 \\times (4.75 - 4)$$\n",
    "\n",
    "$$Q(2, B) \\leftarrow 4 + 0.1 \\times 0.75$$\n",
    "\n",
    "$$Q(2, B) \\leftarrow 4 + 0.075 = 4.075$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda7a2b-b3e3-4537-aaa2-4f1af88cd9af",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "**Given:**\n",
    "- Linear approximator: $Q(s, a) \\approx w_0 + w_1 s + w_2 a$\n",
    "- $\\mathbf{w} = [1, 0.5, -0.2]$\n",
    "- Sample: $(s = 3, a = 2, r = 4, s' = 5)$\n",
    "- $\\gamma = 0.9$\n",
    "- Target: $\\max_{a'} Q(s', a') = 10$\n",
    "- Learning rate: $\\alpha = 0.05$\n",
    "\n",
    "**Task:** Compute the target value $y$ and perform one gradient descent step\n",
    "\n",
    "### Solution:\n",
    "\n",
    "**Step 1: Compute current $Q(s, a)$**\n",
    "\n",
    "$$Q(s, a) = w_0 + w_1 s + w_2 a$$\n",
    "\n",
    "$$Q(3, 2) = 1 + 0.5 \\times 3 + (-0.2) \\times 2$$\n",
    "\n",
    "$$Q(3, 2) = 1 + 1.5 - 0.4 = 2.1$$\n",
    "\n",
    "**Step 2: Compute target value $y$**\n",
    "\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "$$y = 4 + 0.9 \\times 10$$\n",
    "\n",
    "$$y = 4 + 9 = 13$$\n",
    "\n",
    "**Step 3: Compute gradient**\n",
    "\n",
    "Prediction error:\n",
    "\n",
    "$$\\delta = Q(s, a) - y = 2.1 - 13 = -10.9$$\n",
    "\n",
    "Gradient of Q with respect to weights:\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w_0} = 1, \\quad \\frac{\\partial Q}{\\partial w_1} = s = 3, \\quad \\frac{\\partial Q}{\\partial w_2} = a = 2$$\n",
    "\n",
    "Gradient of loss:\n",
    "\n",
    "$$\\nabla L = \\delta \\times [1, s, a] = -10.9 \\times [1, 3, 2]$$\n",
    "\n",
    "$$\\nabla L = [-10.9, -32.7, -21.8]$$\n",
    "\n",
    "**Step 4: Gradient descent update**\n",
    "\n",
    "$$\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\alpha \\times \\nabla L$$\n",
    "\n",
    "$$\\mathbf{w}_{\\text{new}} = [1, 0.5, -0.2] - 0.05 \\times [-10.9, -32.7, -21.8]$$\n",
    "\n",
    "$$\\mathbf{w}_{\\text{new}} = [1, 0.5, -0.2] - [-0.545, -1.635, -1.09]$$\n",
    "\n",
    "$$\\mathbf{w}_{\\text{new}} = [1 + 0.545, 0.5 + 1.635, -0.2 + 1.09]$$\n",
    "\n",
    "$$\\mathbf{w}_{\\text{new}} = [1.545, 2.135, 0.89]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d747e97-9178-4029-b5b1-185c5f6b764e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
